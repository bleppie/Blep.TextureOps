#include "TextureCommon.hlsl"

Texture2D<float4> SrcA;
Texture2D<float4> SrcB;
float4 ScalarA;
float4 ScalarB;
RWTexture2D<float4> Dst;
RWStructuredBuffer<uint4> Histogram;

// -------------------------------------------------------------------------------

// Converts color in linear space  to Unity gamma space
inline float4 Linear2Gamma(float4 color) {
    // Approximation
    // from http://chilliant.blogspot.com.au/2012/08/srgb-approximations-for-hlsl.html?m=1
    color.rgb = max(color.rgb, float3(0, 0, 0));
    color.rgb = max(1.055 * pow(color.rgb, 0.416666667) - 0.055, 0);
    return color;
}

// Converts color in Unity gamma space to linear space
inline float4 Gamma2Linear(float4 color) {
    // Approximate
    // From https://web.archive.org/web/20200207113336/http://lolengine.net/blog/2013/07/27/rgb-to-hsv-in-glsl
    color.rgb = color.rgb * (color.rgb * (color.rgb * 0.305306011 + 0.682171111) + 0.012522878);
    return color;
}

inline float4 RGB2HSV(float4 c) {
    // From https://web.archive.org/web/20200207113336/http://lolengine.net/blog/2013/07/27/rgb-to-hsv-in-glsl
    float4 K = float4(0.0f, -1.0f / 3.0f, 2.0f / 3.0f, -1.0f);
    float4 p = lerp(float4(c.bg, K.wz), float4(c.gb, K.xy), step(c.b, c.g));
    float4 q = lerp(float4(p.xyw, c.r), float4(c.r, p.yzx), step(p.x, c.r));

    float d = q.x - min(q.w, q.y);
    float e = 1.0e-10f;
    return float4(abs(q.z + (q.w - q.y) / (6.0 * d + e)), d / (q.x + e), q.x, c.a);
}

inline float4 HSV2RGB(float4 c) {
    //  https://gamedev.stackexchange.com/questions/59797/glsl-shader-change-hue-saturation-brightness
    float4 K = float4(1.0f, 2.0f / 3.0f, 1.0f / 3.0f, 3.0f);
    float3 p = abs(frac(c.xxx + K.xyz) * 6.0f - K.www);
    return float4(c.z * lerp(K.xxx, saturate(p - K.xxx), c.y), c.a);
}

float4 Grayscale(float4 color) {
    float lum = dot(color.rgb, float3(0.2126, 0.7152, 0.0722));
    return float4(lum, lum, lum, color.a);
}

float4 GrayscaleGamma(float4 color) {
    color = Gamma2Linear(color);
    float lum = dot(color.rgb, float3(0.2126, 0.7152, 0.0722));
    return float4(lum, lum, lum, color.a);
}

float4 Swizzle(float4 color, uint4 channels) {
    return float4(color[channels.x],
                  color[channels.y],
                  color[channels.z],
                  color[channels.w]);
}

float4 Lookup(float4 color, Texture2D<float4> pallete) {
    return float4(SampleLinear(pallete, float2(color.r, 0)).r,
                  SampleLinear(pallete, float2(color.g, 0)).g,
                  SampleLinear(pallete, float2(color.b, 0)).b,
                  SampleLinear(pallete, float2(color.a, 0)).a);
}


// -------------------------------------------------------------------------------
// Color conversion

#pragma kernel Grayscale
[numthreads(THREADSX, THREADSY, 1)]
void Grayscale(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = Grayscale(SrcA[xy]);
    }
}

#pragma kernel GrayscaleI
[numthreads(THREADSX, THREADSY, 1)]
void GrayscaleI(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = Grayscale(Dst[xy]);
    }
}

// Convert to linear, then grayscale, then back to gamma
// See https://en.wikipedia.org/wiki/Grayscale#Converting_color_to_grayscale
#pragma kernel GrayscaleGamma
[numthreads(THREADSX, THREADSY, 1)]
void GrayscaleGamma(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = GrayscaleGamma(SrcA[xy]);
    }
}

// Convert to linear, then grayscale, then back to gamma
// See https://en.wikipedia.org/wiki/Grayscale#Converting_color_to_grayscale
#pragma kernel GrayscaleGammaI
[numthreads(THREADSX, THREADSY, 1)]
void GrayscaleGammaI(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = GrayscaleGamma(Dst[xy]);
    }
}

#pragma kernel Threshold
[numthreads(THREADSX, THREADSY, 1)]
void Threshold(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = step(ScalarA, SrcA[xy]);
    }
}

#pragma kernel ThresholdI
[numthreads(THREADSX, THREADSY, 1)]
void ThresholdI(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = step(ScalarA, Dst[xy]);
    }
}

#pragma kernel ConvertRGB2HSV
[numthreads(THREADSX, THREADSY, 1)]
void ConvertRGB2HSV(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = RGB2HSV(SrcA[xy]);
    }
}

#pragma kernel ConvertRGB2HSVI
[numthreads(THREADSX, THREADSY, 1)]
void ConvertRGB2HSVI(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = RGB2HSV(Dst[xy]);
    }
}

#pragma kernel ConvertHSV2RGB
[numthreads(THREADSX, THREADSY, 1)]
void ConvertHSV2RGB(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = HSV2RGB(SrcA[xy]);
    }
}

#pragma kernel ConvertHSV2RGBI
[numthreads(THREADSX, THREADSY, 1)]
void ConvertHSV2RGBI(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = HSV2RGB(Dst[xy]);
    }
}

#pragma kernel Swizzle
[numthreads(THREADSX, THREADSY, 1)]
void Swizzle(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = Swizzle(SrcA[xy], uint4(ScalarA));
    }
}

#pragma kernel SwizzleI
[numthreads(THREADSX, THREADSY, 1)]
void SwizzleI(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = Swizzle(Dst[xy], uint4(ScalarA));
    }
}

// Use color values in SrcA as indicies into pallette SrcB
#pragma kernel Lookup
[numthreads(THREADSX, THREADSY, 1)]
void Lookup(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = Lookup(SrcA[xy], SrcB);
    }
}

// Use color values in SrcA as indicies into pallette SrcB
#pragma kernel LookupI
[numthreads(THREADSX, THREADSY, 1)]
void LookupI(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = Lookup(Dst[xy], SrcB);
    }
}

// -------------------------------------------------------------------------------
// Geometric

inline void _SwapDst(uint2 xy1, uint2 xy2) {
    float4 tmp = Dst[xy1];
    Dst[xy1] = Dst[xy2];
    Dst[xy2] = tmp;
}

#pragma kernel FlipHorizontal
[numthreads(THREADSX, THREADSY, 1)]
void FlipHorizontal(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = SrcA[uint2(TextureSize.x - xy.x - 1, xy.y)];
    }
}

#pragma kernel FlipHorizontalI
[numthreads(THREADSX, THREADSY, 1)]
void FlipHorizontalI(uint2 xy : SV_DispatchThreadID) {
    if (all(xy < uint2((TextureSize.x + 1) / 2, TextureSize.y))) {
        _SwapDst(xy, uint2(TextureSize.x - xy.x - 1, xy.y));
    }
}

[numthreads(THREADSX, THREADSY, 1)]
#pragma kernel FlipVertical
void FlipVertical(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = SrcA[uint2(xy.x, TextureSize.y - xy.y - 1)];
    }
}

[numthreads(THREADSX, THREADSY, 1)]
#pragma kernel FlipVerticalI
void FlipVerticalI(uint2 xy : SV_DispatchThreadID) {
    if (all(xy < uint2(TextureSize.x, (TextureSize.y + 1) / 2))) {
        _SwapDst(xy, uint2(xy.x, TextureSize.y - xy.y - 1));
    }
}

[numthreads(THREADSX, THREADSY, 1)]
#pragma kernel Rotate180
void Rotate180(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = SrcA[TextureSize - xy - 1];
    }
}

[numthreads(THREADSX, THREADSY, 1)]
#pragma kernel Rotate180I
void Rotate180I(uint2 xy : SV_DispatchThreadID) {
    if (all(xy < uint2(TextureSize.x, (TextureSize.y + 1) / 2))) {
        _SwapDst(xy, TextureSize - xy - 1);
    }
}

// -------------------------------------------------------------------------------
// Misc/Experimental

#pragma kernel DistanceTransformInit
[numthreads(THREADSX, THREADSY, 1)]
void DistanceTransformInit(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        // distanceSq, x, y, value
        // For zero pixels, use size.x*size.x + size.y+size.y as the maximum
        // It's a good indicator and usable if jump flooding fails.
        float value = SrcA[xy].r;
        Dst[xy] = float4(value > 0 ? 0 : dot(TextureSize, TextureSize), xy, value);
    }
}

void DistanceMin(uint2 xy, int2 offset, inout float4 minInfo) {
    float4 offsetInfo = SrcA[clamp(xy + offset, 0, TextureSize - 1)];

    // If has valid value
    if (offsetInfo.w > 0) {
        // Get squared distance to closest
        float2 delta = xy - offsetInfo.yz; // coordinates are in y,z
        float distanceSq = dot(delta, delta);
        if (distanceSq < minInfo.x){
            minInfo = float4(distanceSq, offsetInfo.yzw);
        }
    }
}

#pragma kernel DistanceTransformStep
[numthreads(THREADSX, THREADSY, 1)]
void DistanceTransformStep(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        int offset = (uint) ScalarA.x;
        float4 minInfo = SrcA[xy];
        DistanceMin(xy, int2(-offset, -offset), minInfo);
        DistanceMin(xy, int2(-offset,       0), minInfo);
        DistanceMin(xy, int2(-offset,  offset), minInfo);
        DistanceMin(xy, int2(      0, -offset), minInfo);
        DistanceMin(xy, int2(      0,  offset), minInfo);
        DistanceMin(xy, int2( offset, -offset), minInfo);
        DistanceMin(xy, int2( offset,       0), minInfo);
        DistanceMin(xy, int2( offset,  offset), minInfo);
        Dst[xy] = minInfo;
    }
}

#pragma kernel DistanceTransformSqrt
[numthreads(THREADSX, THREADSY, 1)]
void DistanceTransformSqrt(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        Dst[xy] = float4(sqrt(Dst[xy].x), Dst[xy].yzw);
    }
}

// -------------------------------------------------------------------------------
// Morphology etc

// Gather src pixels into groupshared memory before accessing.
//
// This is the same or slower with 32x32 threads than the simple approach on
// OSX/Metal! Lowering the number of threads to 16x16 makes this about 25%
// faster than the simple approach with 16x16 theads, but both are still slower
// than 32x32 threads and the simple apprach. On a 4096x4096 image:
//    32x32 Simple : 807 Hz *best*
//          Gather : 586 Hz
//    16x16 Simple : 553 Hz
//          Gather : 720 Hz
//     8x 8 Simple : 543 Hz
//          Gather : 617 Hz

groupshared float4 SrcGroup[THREADSY][THREADSX];

#pragma kernel ErodeGather
[numthreads(THREADSX, THREADSY, 1)]
void ErodeGather(uint2 group: SV_GroupID,
                 uint2 xyGroup: SV_GroupThreadID,
                 uint2 id : SV_DispatchThreadID) {

    // Overlap tiles by 1 pixel each (2 pixels overlap), so process in groups of THREADS-2

    // Offset xy to account for overlapping
    uint2 xy = id - 1 - group;
    float4 CTR = SrcA[clamp(xy, 0, TextureSize - 1)];

    // Gather samples into groupshared memory
    uint x = xyGroup.x;
    uint y = xyGroup.y;
    SrcGroup[y][x] = CTR;

    // Wait for all threads to finish populating SrcGroup
    GroupMemoryBarrierWithGroupSync();

    // Process pixels, ignoring those on the border of the tile
    if (all(xy < TextureSize &&
            xyGroup > 0 && xyGroup < uint2(THREADSX-1, THREADSY-1))) {

        CTR = min(CTR, SrcGroup[y - 1][x - 1]);
        CTR = min(CTR, SrcGroup[y - 1][x    ]);
        CTR = min(CTR, SrcGroup[y - 1][x + 1]);
        CTR = min(CTR, SrcGroup[y    ][x - 1]);
        CTR = min(CTR, SrcGroup[y    ][x + 1]);
        CTR = min(CTR, SrcGroup[y + 1][x - 1]);
        CTR = min(CTR, SrcGroup[y + 1][x    ]);
        CTR = min(CTR, SrcGroup[y + 1][x + 1]);
        Dst[xy] = CTR;
    }
}

#pragma kernel Erode
[numthreads(THREADSX, THREADSY, 1)]
void Erode(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        uint2 xySE = max(0, xy - 1);
        uint2 xyNW = min(TextureSize - 1, xy + 1);

        float4 SE = SrcA[xySE];
        float4 SO = SrcA[uint2(xy  .x, xySE.y)];
        float4 SW = SrcA[uint2(xySE.x, xySE.y)];
        float4 EA = SrcA[uint2(xySE.x, xy  .y)];
        float4 WE = SrcA[uint2(xyNW.x, xy  .y)];
        float4 NE = SrcA[uint2(xySE.x, xyNW.y)];
        float4 NO = SrcA[uint2(xy  .x, xyNW.y)];
        float4 NW = SrcA[xyNW];

        float4 nbrMin = min(min(min(SE, SO), min(SW, EA)),
                            min(min(WE, NE), min(NO, NW)));
        Dst[xy] = min(SrcA[xy], nbrMin);
    }
}

#pragma kernel Dilate
[numthreads(THREADSX, THREADSY, 1)]
void Dilate(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        uint2 xySE = max(0, xy - 1);
        uint2 xyNW = min(TextureSize - 1, xy + 1);

        float4 SE = SrcA[xySE];
        float4 SO = SrcA[uint2(xy  .x, xySE.y)];
        float4 SW = SrcA[uint2(xySE.x, xySE.y)];
        float4 EA = SrcA[uint2(xySE.x, xy  .y)];
        float4 WE = SrcA[uint2(xyNW.x, xy  .y)];
        float4 NE = SrcA[uint2(xySE.x, xyNW.y)];
        float4 NO = SrcA[uint2(xy  .x, xyNW.y)];
        float4 NW = SrcA[xyNW];

        float4 nbrMax = max(max(max(SE, SO), max(SW, EA)),
                            max(max(WE, NE), max(NO, NW)));
        Dst[xy] = max(SrcA[xy], nbrMax);
    }
}

// A Fast Parallel Algorithm for Thinning Digital Patterns, T. Y. Zhang and C. Y. Suen
// Reference implementation here: https://github.com/dingmyu/thinning_Zhan-Suen
#pragma kernel Skeletonize
[numthreads(THREADSX, THREADSY, 1)]
void Skeletonize(uint2 xy : SV_DispatchThreadID) {
    // Ignore outermost pixels
    if (all(xy > uint2(0, 0) && xy < TextureSize - uint2(1,1))) {
        // Get 8-neighbours
        uint SE = uint(SrcA[xy + int2(-1, -1)].r);
        uint SO = uint(SrcA[xy + int2( 0, -1)].r);
        uint SW = uint(SrcA[xy + int2( 1, -1)].r);
        uint EA = uint(SrcA[xy + int2(-1,  0)].r);
        uint WE = uint(SrcA[xy + int2( 1,  0)].r);
        uint NE = uint(SrcA[xy + int2(-1,  1)].r);
        uint NO = uint(SrcA[xy + int2( 0,  1)].r);
        uint NW = uint(SrcA[xy + int2( 1,  1)].r);

        // Arrange in clockwise order
        uint neighbors = (EA << 7 | NE << 6 | NO << 5 | NW << 4 | WE << 3 | SW << 2 | SO << 1 | SE);

        // How many neighbors are on
        uint count = countbits(neighbors);

        // How many transitions from off to on
        // Left rotate bits by one, xor with original bits, and and with rotated-bits
        uint neighborsRotated = ((neighbors << 1) & 0xff) | (neighbors >> 7);
        uint transitions = (neighbors ^ neighborsRotated) & neighborsRotated;
        uint transitionCount = countbits(transitions);

        // Weird neighbor test, different for each iteration
        // Iter ? (EA * SO * NO == 0) && (EA * SO * WE == 0)
        //      : (NO * WE * EA == 0) && (NO * WE * SO == 0)
        bool iter = ScalarA.x > 0;
        bool neighborTest = iter
            ? ! all(uint3(EA, SO, NO + WE))
            : ! all(uint3(NO, WE, EA + SO));

        // If all tests pass, turn off the pixel
        if (transitionCount == 1 && count >= 2 && count <= 6 && neighborTest) {
            Dst[xy] = 0;
        }
        else {
            Dst[xy] = SrcA[xy].r;
        }
    }
}

#pragma kernel Sobel
[numthreads(THREADSX, THREADSY, 1)]
void Sobel(uint2 xy : SV_DispatchThreadID) {
    // Ignore outermost pixels
    if (all(xy > uint2(0, 0) && xy < TextureSize - uint2(1,1))) {
        // Get 8-neighbours
        float4 SE = SrcA[xy + int2(-1, -1)];
        float4 SO = SrcA[xy + int2( 0, -1)];
        float4 SW = SrcA[xy + int2( 1, -1)];
        float4 EA = SrcA[xy + int2(-1,  0)];
        float4 WE = SrcA[xy + int2( 1,  0)];
        float4 NE = SrcA[xy + int2(-1,  1)];
        float4 NO = SrcA[xy + int2( 0,  1)];
        float4 NW = SrcA[xy + int2( 1,  1)];

        float4 dx = (NW - NE) + 2 * (WE - EA) + (SW - SE);
        float4 dy = (NW - SW) + 2 * (NO - SO) + (NE - SE);

        Dst[xy] = saturate(sqrt(dx * dx + dy * dy));
    }
}

#pragma kernel Scharr
[numthreads(THREADSX, THREADSY, 1)]
void Scharr(uint2 xy : SV_DispatchThreadID) {
    // Ignore outermost pixels
    if (all(xy > uint2(0, 0) && xy < TextureSize - uint2(1,1))) {

        // Get 8-neighbours
        float4 SE = SrcA[xy + int2(-1, -1)];
        float4 SO = SrcA[xy + int2( 0, -1)];
        float4 SW = SrcA[xy + int2( 1, -1)];
        float4 EA = SrcA[xy + int2(-1,  0)];
        float4 WE = SrcA[xy + int2( 1,  0)];
        float4 NE = SrcA[xy + int2(-1,  1)];
        float4 NO = SrcA[xy + int2( 0,  1)];
        float4 NW = SrcA[xy + int2( 1,  1)];

        float4 dx = 3 * (NW - NE + SW - SE) + 10 * (WE - EA);
        float4 dy = 3 * (NW - SW + NE - SE) + 10 * (NO - SO);

        Dst[xy] = saturate(sqrt(dx * dx + dy * dy));
    }
}

// -------------------------------------------------------------------------------
// Median
// Morgan McGuire and Kyle Whitson, http://graphics.cs.williams.edu, see Copyright

#define s2(a, b)          tmp = a; a = min(a, b); b = max(tmp, b);
#define mn3(a, b, c)      s2(a, b); s2(a, c);
#define mx3(a, b, c)      s2(b, c); s2(a, c);

#define mnmx3(a, b, c)          mx3(a, b, c); s2(a, b);                                   // 3 exchanges
#define mnmx4(a, b, c, d)       s2(a, b); s2(c, d); s2(a, c); s2(b, d);                   // 4 exchanges
#define mnmx5(a, b, c, d, e)    s2(a, b); s2(c, d); mn3(a, c, e); mx3(b, d, e);           // 6 exchanges
#define mnmx6(a, b, c, d, e, f) s2(a, d); s2(b, e); s2(c, f); mn3(a, b, c); mx3(d, e, f); // 7 exchanges

#define t2(a, b)                          s2(v[a], v[b]);
#define t24(a, b, c, d, e, f, g, h)       t2(a, b); t2(c, d); t2(e, f); t2(g, h);
#define t25(a, b, c, d, e, f, g, h, i, j) t24(a, b, c, d, e, f, g, h); t2(i, j);

#pragma kernel Median3x3
[numthreads(THREADSX, THREADSY, 1)]
void Median3x3(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        float4 v[6];
        float4 tmp;

        v[0] = SrcA[ClampToRange(xy + uint2(-1, -1))];
        v[1] = SrcA[ClampToRange(xy + uint2( 0, -1))];
        v[2] = SrcA[ClampToRange(xy + uint2( 1, -1))];
        v[3] = SrcA[ClampToRange(xy + uint2(-1,  0))];
        v[4] = SrcA[ClampToRange(xy + uint2( 0,  0))];
        v[5] = SrcA[ClampToRange(xy + uint2( 1,  0))];
        mnmx6(v[0], v[1], v[2], v[3], v[4], v[5]);

        v[5] = SrcA[ClampToRange(xy + uint2(-1,  1))];
        mnmx5(v[1], v[2], v[3], v[4], v[5]);

        v[5] = SrcA[ClampToRange(xy + uint2( 0,  1))];
        mnmx4(v[2], v[3], v[4], v[5]);

        v[5] = SrcA[ClampToRange(xy + uint2( 1,  1))];
        mnmx3(v[3], v[4], v[5]);
        Dst[xy] = v[4];
    }
}

// Reduce the number of threads, otherwise we run out of registers (warning X4714)
#pragma kernel Median5x5
[numthreads(THREADSX/2, THREADSY/2, 1)]
void Median5x5(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        float4 v[25];
        uint i = 0;
        for (int y = -2; y <= 2; y++) {
            v[i++] = SrcA[ClampToRange(xy + int2(-2, y))];
            v[i++] = SrcA[ClampToRange(xy + int2(-1, y))];
            v[i++] = SrcA[ClampToRange(xy + int2( 0, y))];
            v[i++] = SrcA[ClampToRange(xy + int2( 1, y))];
            v[i++] = SrcA[ClampToRange(xy + int2( 2, y))];
        }

        float4 tmp;
        t25( 0,  1,    3,  4,    2,  4,    2,  3,    6,  7);
        t25( 5,  7,    5,  6,    9,  7,    1,  7,    1,  4);
        t25(12, 13,   11, 13,   11, 12,   15, 16,   14, 16);
        t25(14, 15,   18, 19,   17, 19,   17, 18,   21, 22);
        t25(20, 22,   20, 21,   23, 24,    2,  5,    3,  6);
        t25( 0,  6,    0,  3,    4,  7,    1,  7,    1,  4);
        t25(11, 14,    8, 14,    8, 11,   12, 15,    9, 15);
        t25( 9, 12,   13, 16,   10, 16,   10, 13,   20, 23);
        t25(17, 23,   17, 20,   21, 24,   18, 24,   18, 21);
        t25(19, 22,    8, 17,    9, 18,    0, 18,    0, 9);
        t25(10, 19,    1, 19,    1, 10,   11, 20,    2, 20);
        t25( 2, 11,   12, 21,    3, 21,    3, 12,   13, 22);
        t25( 4, 22,    4, 13,   14, 23,    5, 23,    5, 14);
        t25(15, 24,    6, 24,    6, 15,    7, 16,    7, 19);
        t25( 3, 11,    5, 17,   11, 17,    9, 17,    4, 10);
        t25( 6, 12,    7, 14,    4,  6,    4,  7,   12, 14);
        t25(10, 14,    6,  7,   10, 12,    6, 10,    6, 17);
        t25(12, 17,    7, 17,    7, 10,   12, 18,    7, 12);
        t24(10, 18,   12, 20,   10, 20,   10, 12);

        Dst[xy] = v[12];
    }
}

// -------------------------------------------------------------------------------
// Blurring

// Incremental gaussian
// TODO/SPEED: Transpose after first pass?
// https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-40-incremental-computation-gaussian
#pragma kernel BlurGaussian
[numthreads(THREADSX, THREADSY, 1)]
void BlurGaussian(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        float2 uv = GetUV(xy);

        float3 incGauss = ScalarA.xyz;
        float size = ScalarA.w;
        float2 direction = ScalarB.xy;

        float4 color = SamplePoint(SrcA, uv) * incGauss.x;
        float  sum = incGauss.x;
        incGauss.xy *= incGauss.yz;

        float2 delta = direction * TexelSize.xy;
        int halfSize = int(ceil((size - 1) * 0.5));
        for (int i = 1; i <= halfSize; i++) {
            float2 offset = i * delta;
            color += incGauss.x * (SamplePoint(SrcA, uv - offset) +
                                   SamplePoint(SrcA, uv + offset));
            sum += 2 * incGauss.x;
            incGauss.xy *= incGauss.yz;
        }

        Dst[xy] = color / sum;

    }
}

// Recursive convolution
// see https://www.researchgate.net/publication/222453003_Recursive_implementation_of_the_Gaussian_filter

// Unused slow method.
[numthreads(THREADSX * THREADSY, 1, 1)]
void RecursiveConvolve(uint2 xy : SV_DispatchThreadID) {
    uint rowOrCol = xy.x;

    float4 coeffs = ScalarA;
    int2 direction = int2(ScalarB.xy);

    bool isForward = any(direction > 0);
    bool isHorizontal = direction.y == 0;

    if (rowOrCol < (isHorizontal ? TextureSize.y : TextureSize.x)) {

        uint count = isHorizontal ? TextureSize.x : TextureSize.y;
        uint start = isForward ? 0 : count - 1;
        uint2 idx = isHorizontal ? uint2(start, rowOrCol) : uint2(rowOrCol, start);

        float4x4 prevM = { SrcA[idx], SrcA[idx], SrcA[idx], SrcA[idx] };

        for (uint i = 0; i < count; ++i, idx += direction) {

            prevM[0] = SrcA[idx];
            float4 val = mul(coeffs, prevM);
            prevM[3] = prevM[2];
            prevM[2] = prevM[1];
            prevM[1] = val;

            Dst[idx] = val;
        }
    }
}

// Run through each row forwards
#pragma kernel RecursiveConvolveFwd
[numthreads(THREADSX * THREADSY, 1, 1)]
void RecursiveConvolveFwd(uint y : SV_DispatchThreadID) {
    if (y < TextureSize.y) {

        float4 coeffs = ScalarA;
        uint2 idx = uint2(0, y);
        uint count = TextureSize.x;

        float4x4 prevM = { SrcA[idx], SrcA[idx], SrcA[idx], SrcA[idx] };

        for (uint i = 0; i < count; ++i, idx += uint2(1, 0)) {
            prevM[0] = SrcA[idx];
            float4 val = mul(coeffs, prevM);
            prevM[3] = prevM[2];
            prevM[2] = prevM[1];
            prevM[1] = val;

            Dst[idx] = val;
        }
    }
}

// Run through each row forwards, inplace
#pragma kernel RecursiveConvolveFwdI
[numthreads(THREADSX * THREADSY, 1, 1)]
void RecursiveConvolveFwdI(uint y : SV_DispatchThreadID) {
    if (y < TextureSize.y) {

        float4 coeffs = ScalarA;
        uint2 idx = uint2(0, y);
        uint count = TextureSize.x;

        float4x4 prevM = { Dst[idx], Dst[idx], Dst[idx], Dst[idx] };

        for (uint i = 0; i < count; ++i, idx += uint2(1, 0)) {
            prevM[0] = Dst[idx];
            float4 val = mul(coeffs, prevM);
            prevM[3] = prevM[2];
            prevM[2] = prevM[1];
            prevM[1] = val;

            Dst[idx] = val;
        }
    }
}

// Run through each row backwards and traspose output
#pragma kernel RecursiveConvolveBak
[numthreads(THREADSX * THREADSY, 1, 1)]
void RecursiveConvolveBak(uint y : SV_DispatchThreadID) {
    if (y < TextureSize.y) {

        float4 coeffs = ScalarA;
        uint2 idx = uint2(TextureSize.x - 1, y);
        uint count = TextureSize.x;

        float4x4 prevM = { SrcA[idx], SrcA[idx], SrcA[idx], SrcA[idx] };

        for (uint i = 0; i < count; ++i, idx -= uint2(1, 0)) {
            prevM[0] = SrcA[idx];
            float4 val = mul(coeffs, prevM);
            prevM[3] = prevM[2];
            prevM[2] = prevM[1];
            prevM[1] = val;

            // Transpose
            Dst[idx.yx] = val;
        }
    }
}

// -------------------------------------------------------------------------------
// Histogram

#pragma kernel HistogramEqClear
[numthreads(256, 1, 1)]
void HistogramEqClear(uint x : SV_DispatchThreadID) {
    Histogram[x] = uint4(0, 0, 0, 0);
}

// TODO/SPEED: Isolate threads to prevent collisions
#pragma kernel HistogramEqGather
[numthreads(THREADSX, THREADSY, 1)]
void HistogramEqGather(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        uint4 col = uint4(255.999 * SrcA[xy]);
        InterlockedAdd(Histogram[col.r].r, 1);
        InterlockedAdd(Histogram[col.g].g, 1);
        InterlockedAdd(Histogram[col.b].b, 1);
        InterlockedAdd(Histogram[col.a].a, 1);
    }
}

#pragma kernel HistogramEqAccumulate
[numthreads(1, 1, 1)]
//[numthreads(256, 1, 1)]
void HistogramEqAccumulate(uint x : SV_DispatchThreadID) {

    for (uint d = 1; d < 256; d++) {
        Histogram[d] += Histogram[d - 1];
    }

    // GroupMemoryBarrierWithGroupSync doesn't seem to work on Metal (?), so do it serially.
    // Recursive doubling algorithm: On the first pass, the value one element to
    // the left is added to the current value. On the second pass, the value two
    // elements to the left is added the current value. In general, the stride
    // is doubled for each pass. See:
    // https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.8836

    //for (uint d = 1; d <= 128; d <<= 1) {
    //  if (id.x >= d) {
    //    Histogram[id.x] += Histogram[id.x - d];
    //  }
    //}
    //GroupMemoryBarrierWithGroupSync();
}

#pragma kernel HistogramEqMap
[numthreads(THREADSX, THREADSY, 1)]
void HistogramEqMap(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        float4 col = SrcA[xy];
        uint4 col255 = uint4(255.999f * col);
        float3 colMapped = float3(Histogram[col255.r].r,
                                  Histogram[col255.g].g,
                                  Histogram[col255.b].b) * (TexelSize.x * TexelSize.y);
        Dst[xy] = float4(colMapped, col.a);
    }
}

// -------------------------------------------------------------------------------
// Stats, these all work in place

#pragma kernel MaxReduce
[numthreads(THREADSX, THREADSY, 1)]
void MaxReduce(uint2 xy : SV_DispatchThreadID) {
    uint2 offset = (TextureSize + 1) >> 1;
    if (all(xy < offset)) {
        uint2 last = TextureSize - 1;

        float4 WE = Dst[min(xy + uint2(offset.x, 0), last)];
        float4 NO = Dst[min(xy + uint2(0, offset.y), last)];
        float4 NW = Dst[min(xy + offset            , last)];

        Dst[xy] = max(max(Dst[xy], WE), max(NO, NW));
    }
}

#pragma kernel MinReduce
[numthreads(THREADSX, THREADSY, 1)]
void MinReduce(uint2 xy : SV_DispatchThreadID) {
    uint2 offset = (TextureSize + 1) >> 1;
    if (all(xy < offset)) {
        uint2 last = TextureSize - 1;

        float4 WE = Dst[min(xy + uint2(offset.x, 0), last)];
        float4 NO = Dst[min(xy + uint2(0, offset.y), last)];
        float4 NW = Dst[min(xy + offset            , last)];

        Dst[xy] = min(min(Dst[xy], WE), min(NO, NW));
    }
}

#pragma kernel SumReduce
[numthreads(THREADSX, THREADSY, 1)]
void SumReduce(uint2 xy : SV_DispatchThreadID) {
    uint2 offset = (TextureSize + 1) >> 1;
    if (all(xy < offset)) {

        uint2 xyNW = xy + offset;
        float4 WE = xyNW.x   < TextureSize.x ? Dst[uint2(xyNW.x, xy.y)] : 0;
        float4 NO = xyNW.y   < TextureSize.y ? Dst[uint2(xy.x, xyNW.y)] : 0;
        float4 NW = all(xyNW < TextureSize)  ? Dst[xyNW] : 0;

        Dst[xy] = Dst[xy] + WE + NO + NW;
    }
}

// -------------------------------------------------------------------------------
// Composition

// Over
// αA*A+(1- αA)* αB*B
// αA+(1-αA)* αB
// A occludes B
#pragma kernel ComposeOver
[numthreads(THREADSX, THREADSY, 1)]
void ComposeOver(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        float4 colorA = SrcA[xy];
        float4 colorB = SrcB[xy];
        colorA.rgb *= colorA.a;
        colorB.rgb *= colorB.a;
        Dst[xy] = lerp(colorB, float4(colorA.rgb, 1), colorA.a);
    }
}

// IN
// αA*A*αB
// αA*αB
// A within B. B acts as a matte for A. A shows only where B is visible.
#pragma kernel ComposeIn
[numthreads(THREADSX, THREADSY, 1)]
void ComposeIn(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        float4 colorA = SrcA[xy];
        float4 colorB = SrcB[xy];
        colorA.rgb *= colorA.a;
        Dst[xy] = colorA * colorB.a;
    }
}

// OUT
// αA*A*(1-αB)
// αA*(1-αB)
// A outside B. NOT-B acts as a matte for A. A shows only where B is not visible.
#pragma kernel ComposeOut
[numthreads(THREADSX, THREADSY, 1)]
void ComposeOut(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        float4 colorA = SrcA[xy];
        float4 colorB = SrcB[xy];
        colorA.rgb *= colorA.a;
        Dst[xy] = colorA * (1 - colorB.a);
    }
}

// ATOP
// αA*A*αB+(1- αA)* αB*B
// αA*αB+(1- αA)* αB
#pragma kernel ComposeAtop
[numthreads(THREADSX, THREADSY, 1)]
void ComposeAtop(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        float4 colorA = SrcA[xy];
        float4 colorB = SrcB[xy];
        Dst[xy] = float4(lerp(colorB.rgb, colorA.rgb, colorA.a) * colorB.a, colorB.a);
    }
}

// XOR
// αA*A*(1-αB)+(1- αA)* αB*B = lerp(aB * B, A * (1 - aB), aA) = lerp(aA * A, (1-aA)*B, aB)
// αA*(1-αB)+(1- αA)* αB =
// Combination of (A OUT B) and (B OUT A). A and B mutually exclude each other.
#pragma kernel ComposeXor
[numthreads(THREADSX, THREADSY, 1)]
void ComposeXor(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        float4 colorA = SrcA[xy];
        float4 colorB = SrcB[xy];
        colorA.rgb *= colorA.a;
        colorB.rgb *= colorB.a;
        Dst[xy] = saturate(colorA * (1 - colorB.a) + colorB * (1 - colorA.a));
    }
}

// PLUS
// αA*A+αB*B
// αA+αB
// Blend without precedence
#pragma kernel ComposePlus
[numthreads(THREADSX, THREADSY, 1)]
void ComposePlus(uint2 xy : SV_DispatchThreadID) {
    if (InRange(xy)) {
        float4 colorA = SrcA[xy];
        float4 colorB = SrcB[xy];
        colorA.rgb *= colorA.a;
        colorB.rgb *= colorB.a;
        Dst[xy] = saturate(colorA + colorB);
    }
}
